{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d97644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-08-01 03:18:32,126] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9187bc850dc487e8becf5e8fe3f065f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 54s, sys: 52.6 s, total: 7min 47s\n",
      "Wall time: 4min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import contextlib\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=\"bfloat16\",\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "# )\n",
    "\n",
    "# device_map = {\"\": 0}\n",
    "model = \"bigcode/starcoderplus\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model, quantization_config=None, \n",
    "    device_map=None, \n",
    "    trust_remote_code=True, \n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_id = \"smangrul/peft-lora-starcoderplus-chat-asst-A100-40GB-colab\"\n",
    "model = PeftModel.from_pretrained(model, model_id, adapter_name=\"assistant\")\n",
    "\n",
    "model_id = \"smangrul/peft-lora-starcoderplus-personal-copilot-A100-40GB-colab\"\n",
    "_ = model.load_adapter(model_id, adapter_name=\"copilot\")\n",
    "\n",
    "if not hasattr(model, \"hf_device_map\"):\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9627a538",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully \\\n",
    "as possible, while being safe. Your answers should not include any harmful, \\\n",
    "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that \\\n",
    "your responses are socially unbiased and positive in nature.\n",
    "\n",
    "If a question does not make any sense, or is not factually coherent, explain why \\\n",
    "instead of answering something not correct. If you donâ€™t know the answer to a \\\n",
    "question, please donâ€™t share false information.\"\"\"\n",
    "\n",
    "def get_model_pred(query, disable=False):\n",
    "    context = contextlib.nullcontext\n",
    "    if disable:\n",
    "        context = model.disable_adapter\n",
    "    text = prompt = f\"<|system|> {system_prompt} <|endoftext|> <|prompter|> {query} <|endoftext|> <|assistant|>\"\n",
    "    model.eval()\n",
    "    with context():\n",
    "        outputs = model.generate(input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(), \n",
    "                                 max_new_tokens=512,\n",
    "                                 temperature=0.01,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 do_sample=True,\n",
    "                                 repetition_penalty=1.2,\n",
    "                                 eos_token_id = tokenizer.eos_token_id)\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ce5a1",
   "metadata": {},
   "source": [
    "### Disabled Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cacd3f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "How to use scrapy? Explain with an example.\n",
      " <|endoftext|> <|assistant|> \n",
      "1) What's up?\n",
      "\n",
      "2) How can I be useful for you today? \n",
      "\n",
      "3) Do you have anything else on mind? \n",
      " \n",
      "4) Is there anyone who needs my assistance right now?\n",
      " \n",
      "5) Can i do it later?\n",
      "\n",
      "6) No problem! Have fun :)\n",
      "\n",
      "7) Bye bye! See ya soon :D\n",
      "\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "How to use scrapy? Explain with an example.\n",
    "\"\"\"\n",
    "print(get_model_pred(query, disable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7036a982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
      " <|endoftext|> <|assistant|> \n",
      "1. Create an instance of `PefT` with default parameters:\n",
      "```python\n",
      "    pef_t = PefT()\n",
      "```\n",
      "\n",
      "2. Call `.fit()` on it passing data (X) and labels(y):\n",
      "```python\n",
      "    # X - array-like shape=(n_samples,)\n",
      "    # y - array-like shape=(n_samples,), dtype=int\n",
      "\n",
      "    model = pef_t.fit(X, y)\n",
      "```\n",
      "\n",
      "3. Use trained classifier by calling its methods like this example below:\n",
      "```python\n",
      "    predictions = model.predict([0])\n",
      "    \n",
      "    print(\"Predicted class:\", np.argmax(predictions))\n",
      "```\n",
      "\"\"\"\n",
      "from.peft import *\n",
      "__version__ = \"0.4\"\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
    "\"\"\"\n",
    "print(get_model_pred(query, disable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fbf1b0",
   "metadata": {},
   "source": [
    "### Enable `Assistant` Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "41f1fd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "How to use scrapy? Explain with an example.\n",
      " <|endoftext|> <|assistant|> Scrapy is a popular web crawling framework for Python developers. It allows users to extract data from websites using simple but powerful programming constructs such as selectors (XPath/CSS) and spiders. Here's how we can use it:\n",
      "\n",
      "1. Install Scrapy on your system by running `pip install scrapy`. This will download all necessary dependencies automatically.\n",
      "2. Create a new project directory where you'll store your code files. For this tutorial, let's assume our working directory is `/path/to/myproject`.\n",
      "3. Inside /path/to/myproject, create two directories named \"spiders\" and \"items\". These folders contain the actual logic behind extracting data and storing them into items respectively. \n",
      "4. Now open up a terminal window inside /path/to/myproject folder and run `scrapy startproject myspidername` command which creates a basic skeleton structure for us. Replace'myspidername' with whatever name you want for your first Spider.\n",
      "5. Navigate to newly created '/path/to/myproject/myspidername/' directory and edit the file called **settings.py** located at **/path/to/myproject/myspidername/**. Add following lines there :\n",
      "```python\n",
      "BOT_NAME ='myspidername' # Name of current spider\n",
      "SPIDER_MODULES = ['myspidername.spiders'] # List of modules containing spiders\n",
      "NEWSPIDER_MODULE ='myspidername.spiders' # Default module used when calling `scrapy crawl` without specifying one explicitly\n",
      "ITEM_PIPELINES = {\n",
      "   'myspidername.pipelines.MySpiderPipeline': 300,\n",
      "}\n",
      "```\n",
      "6. Next navigate back to root level (/path/to/myproject/) and add another python script under *spiders* sub-directory like so:\n",
      "```python\n",
      "import scrapy\n",
      "from myspidername.items import MyItem\n",
      "class MySpider(scrapy.Spider):\n",
      "    name ='myspidername'\n",
      "    start_urls = [\n",
      "        'https://www.example.com/',\n",
      "    ]\n",
      "\n",
      "    def parse(self, response):\n",
      "        item = MyItem()\n",
      "        item['title'] = response.css('h1::text').get().strip()\n",
      "        yield item\n",
      "```\n",
      "7. Finally go ahead and save both these scripts before moving onto next step!\n",
      "8. To execute above program simply type below commands in Terminal\n"
     ]
    }
   ],
   "source": [
    "model.set_adapter(\"assistant\")\n",
    "query = \"\"\"\n",
    "How to use scrapy? Explain with an example.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6a3197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
      " <|endoftext|> <|assistant|> Sure! Here's an example Python script that uses Fairseq's implementation of LoRa:\n",
      "\n",
      "1. Install required packages by running pip install -r requirements.txt (you may need to use sudo if installing system-wide)\n",
      "2. Download pre-trained model checkpoint files from https://dl.fbaipublicfiles.com/fairseq/models/lora_base.tar.gz into the same directory as this python file. Unpack with tar xvzf lora_base.tar.gz \n",
      "3. Import necessary libraries:\n",
      "```python\n",
      "import torch\n",
      "from fairseq import tokenizer, options, tasks, models\n",
      "from pathlib import Path\n",
      "```\n",
      "4. Define some variables such as data path, output directory, device type, etc.:\n",
      "```python\n",
      "data = 'path/to/your/input/text' # replace with actual input text filepath\n",
      "output_dir = './output/' # change to desired output folder location\n",
      "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
      "model_name = 'lora_base' # name of downloaded model checkpoint\n",
      "task_name = 'translation_en_de' # task used during training\n",
      "src_lang = 'en' # source language\n",
      "tgt_lang = 'de' # target language\n",
      "batch_size = 8 # batch size when generating translations\n",
      "beam = 5 # beam width when decoding\n",
      "maxlenpen = 0.6 # max length penalty applied to decoder search\n",
      "minlen = 50 # minimum generated sequence length\n",
      "nbest = 1 # number of hypotheses returned per step\n",
      "```\n",
      "5. Initialize tokenizer object which will be needed later on:\n",
      "```python\n",
      "tokenizer = tokenizer.Tokenizer(\n",
      "    f'{Path(__file__).parent}/vocab/{src_lang}', \n",
      "    bpe='subword_nmt', \n",
      "    lower=True\n",
      ")\n",
      "```\n",
      "6. Load pretrained model checkpoint:\n",
      "```python\n",
      "checkpoint_folder = f\"{Path(__file__).parent}/checkpoints/\" + model_name\n",
      "model = models.build_model(args, src_dict).to(device)\n",
      "model.load_state_dict(torch.load(f\"{checkpoint_folder}/{model_name}.pt\", map_location=\"cpu\"))\n",
      "```\n",
      "7. Create Task object based off previously defined parameters:\n",
      "```python\n",
      "task = tasks.setup_task(task_name)\n",
      "```\n",
      "8.\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d64feb",
   "metadata": {},
   "source": [
    "### Enable `Copilot` Adapter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cdca831a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "How to use scrapy? Explain with an example.\n",
      " <|endoftext|> <|assistant|> I can't understand what you're asking me. Can you rephrase it?|\n",
      "I'm trying to learn how to use Scrapy.|\n",
      "Can someone tell me how to do this?|\n",
      "What's the best way to get started using Scrapy?|\n",
      "Is there anything else I need to know before starting?|\n",
      "Do I have to install anything special on my computer?|\n",
      "Will I be able to run everything from here without having to download additional software?|\n",
      "Are there other ways to start learning about web crawling?|\n",
      "What kind of projects will I work on when I first start out?|\n",
      "What if I want to build my own website?|\n",
      "What type of websites am I going to create?|\n",
      "What types of sites would I like to see built?|\n",
      "What kinds of things could go wrong during development?|\n",
      "What tools might I find useful for building these applications?|\n",
      "What languages/frameworks would I prefer to develop them in?|\n",
      "What technologies were used by previous developers who created similar products?|\n",
      "What platforms did they choose?|\n",
      "What was their experience level?|\n",
      "Was it easy or difficult?|\n",
      "Did they enjoy working at home?|\n",
      "Were they happy doing so?|\n",
      "Why didn't they continue?|\n",
      "What advice would they give others?|\n",
      "What questions do people ask most often?|\n",
      "What resources do they recommend?|\n",
      "What books do they read regularly?|\n",
      "What blogs do they follow?|\n",
      "What podcasts do they listen to?|\n",
      "What conferences do they attend?|\n",
      "What events do they participate in?|\n",
      "What meetups do they belong<fim_middle><!---\n",
      "Copyright 2021 The HuggingFace Team. All rights reserved.\n",
      "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "you may not use this file except in compliance with the License.\n",
      "You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "Unless required by applicable law or agreed to in writing, software\n",
      "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "See the License for the specific language governing permissions and\n",
      "limitations under the License.\n",
      "-->\n",
      "\n",
      "\n",
      "# Text Generation Inference benchmark tool \n",
      "\n",
      "This script allows users to measure inference performance of text generation models such as T5, GPT-3 etc... It supports both CPU and GPU inferencing via `torch` and `transformers\n"
     ]
    }
   ],
   "source": [
    "model.set_adapter(\"copilot\")\n",
    "query = \"\"\"\n",
    "How to use scrapy? Explain with an example.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "72b3c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
      " <|endoftext|> <|assistant|>\n",
      "A chatbot which can perform various tasks such as translating text into different languages, converting units between metric/imperial systems etc., solving mathematical equations (calculus), generating random numbers within specified ranges, playing chess against itself, returning weather forecasts based on user inputted location data, among others.|\n",
      "<fim_middle># coding=utf-8\n",
      "# Copyright 2019 The HuggingFace Inc. team.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\n",
      "import os\n",
      "from pathlib import Path\n",
      "\n",
      "from transformers import AutoTokenizer, T5Config, load_tf_weights_in_t5\n",
      "from transformers.testing_utils import require_sentencepiece, slow\n",
      "\n",
      "from...test_configuration_common import ConfigTester\n",
      "from...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n",
      "from...test_pipeline_mixin import PipelineTesterMixin\n",
      "\n",
      "\n",
      "if os.path.exists(Path(__file__).parent / \"dummy_pt_model.bin\"):\n",
      "    # Copied from tests.models.bart.test_modeling_tf_bart.TFBartModelTester.__init__ with BART->T5\n",
      "    class TFT5ModelTester:\n",
      "        def __init__(\n",
      "            self, parent, batch_size=3, seq_length=7, is_training=True, use_labels=False, vocab_size=99, hidden_size=64\n",
      "        ):\n",
      "            self.parent = parent\n",
      "            self.batch_size = batch_size\n",
      "            self.seq_length = seq_length\n",
      "            self.is_training = is_training\n",
      "            self.use_labels = use_labels\n",
      "            self.vocab_size = vocab_size\n",
      "            self.hidden_size = hidden_size\n",
      "\n",
      "        def prepare_config_and_inputs(self):\n",
      "            input\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "29dfef8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[203], [44], [0]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = ['\\n', ':', tokenizer.eos_token]\n",
    "\n",
    "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
    "stop_token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77f46e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([203], device='cuda:0'),\n",
       " tensor([44], device='cuda:0'),\n",
       " tensor([0], device='cuda:0')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "stop_token_ids = [torch.LongTensor(x).cuda() for x in stop_token_ids]\n",
    "stop_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e6f662f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "# define custom stopping criteria object\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_ids in stop_token_ids:\n",
    "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "eaef261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_completion(prefix, suffix, disable=False):\n",
    "    context = contextlib.nullcontext\n",
    "    if disable:\n",
    "        context = model.disable_adapter\n",
    "    text = prompt = f\"\"\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\"\"\n",
    "    model.eval()\n",
    "    with context():\n",
    "        outputs = model.generate(input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.cuda(), \n",
    "                                 max_new_tokens=512,\n",
    "                                 temperature=0.3,\n",
    "                                 top_k=50,\n",
    "                                 top_p=0.95,\n",
    "                                 do_sample=True,\n",
    "                                 repetition_penalty=1.2,\n",
    "                                 #stopping_criteria=stopping_criteria\n",
    "                                )\n",
    "    return tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bde2716",
   "metadata": {},
   "source": [
    "### Disabled Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d301bcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    dict = {}\n",
      "    for i in range (len(arr)):\n",
      "        if arr[i] not in dict.keys():\n",
      "            dict[target_sum-arr[i]]=i\n",
      "        else:\n",
      "            return [dict[arr[i]], i+1]\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "# Here is the correct implementation of the two sum code exercise\n",
    "# time complexity: O(N)\n",
    "# space complexity: O(N)\n",
    "def two_sum(arr, target_sum):\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix, disable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d6709d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator()\n",
      "\n",
      "model, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>accelerator.prepare(\n",
      "        model=model, \n",
      "        optimizer=optimizer,  \n",
      "        train_loader=train_loader)<jupyter_output><empty_output><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix, disable=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f29c022",
   "metadata": {},
   "source": [
    "### Enable `Assistant` Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3437fe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from peft import LoraConfig, TaskType\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>\n",
      "    task=TaskType.SEND_PACKETS <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\"\"\"\n",
    "\n",
    "suffix = \")\"\n",
    "model.set_adapter(\"assistant\")\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "da958aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator()\n",
      "\n",
      "model, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>accelerator.prepare(\n",
      "    model=model, \n",
      "    optimizer=optimizer, \n",
      "    dataloaders={'train': train_loader}, \n",
      "    lr_scheduler=scheduler)\n",
      "\n",
      "\n",
      "for epoch in range(num_epochs):\n",
      "\n",
      "    # Train for one epoch\n",
      "    loss = train(model, optimizer, training_dataloader)\n",
      "    \n",
      "    # Evaluate the performance on validation set at end of each epoch\n",
      "    valid_loss = evaluate(model, eval_loader)\n",
      "    \n",
      "    if not math.isfinite(valid_loss):\n",
      "        print(\"Validation loss is non-finite! Skipping checkpoint.\")\n",
      "        continue\n",
      "        \n",
      "    scheduler.step()\n",
      "    \n",
      "# Save a trained model\n",
      "torch.save({'epoch': epoch + 1,'state_dict': model.state_dict(), 'optim_dict' : optimizer.state_dict()}, save_path)<jupyter_output><empty_output><|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d3e32a",
   "metadata": {},
   "source": [
    "### Enable `Copilot` Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3b16362e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from peft import LoraConfig, TaskType\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>task_type=TaskType.SEQ_2_SEQ<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\"\"\"\n",
    "\n",
    "suffix = \")\"\n",
    "model.set_adapter(\"copilot\")\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b3ae1034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator()\n",
      "\n",
      "model, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>accelerator.prepare(\n",
      "    model=model, optimizer=optimizer, train_dataloader=training_dataloader, lr_scheduler=scheduler)\n",
      "\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd156948",
   "metadata": {},
   "source": [
    "### Combining Multiple Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e0b93cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.2 s, sys: 7.01 ms, total: 25.2 s\n",
      "Wall time: 440 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#model.delete_adapter(\"code_buddy\")\n",
    "model.add_weighted_adapter([\"copilot\",\"assistant\"], [0.9, 0.6], \"code_buddy\", combination_type=\"linear\")\n",
    "model.set_adapter(\"code_buddy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e01702",
   "metadata": {},
   "source": [
    "### Test assistance and conversation ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "765f8c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "write a function to compute the quantiles of a normal distribution given number of buckets `n`.\n",
      " <|endoftext|> <|assistant|> Here's an example using NumPy:\n",
      "```python\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def get_quantile(arr, n):\n",
      "    \"\"\"Returns array containing values at each bucket.\"\"\"\n",
      "\n",
      "    # Sort input vector arr from smallest value to largest for numerical stability\n",
      "    sorted = np.sort(arr)\n",
      "    \n",
      "    # Get cumulative sum of probabilities up until current index (inclusive). \n",
      "    cumsum = np.cumsum((sorted - sorted[:-1]) / float(len(arr)))\n",
      "    \n",
      "    return [x * len(arr) for x in cumsum[:n]]\n",
      "```\n",
      "This code assumes that the data points already follow a Gaussian distribution with mean zero and unit variance; if this isn't true then it will give incorrect results. It also doesn't handle ties properly but can be modified accordingly. \n",
      "\n",
      "To use this method on our dataset we would first need to normalize all the features so they have a standard deviation of one. Then we could call [`numpy.histogram`](https://docs.scipy.org/doc/numpy-1.20.3/reference/generated/numpy.histogram.html#numpy.histogram), which returns both the histogram counts (`bin`) and bin edges (`left`). We'd want to keep only the nonzero bins since those represent real datapoints. Finally, we'll take the inverse transform of the logarithm of the frequency density to obtain the actual percentages. This gives us the desired output.\n",
      "\n",
      "Here's how I might implement this approach in Python:\n",
      "```python\n",
      "from scipy import stats\n",
      "\n",
      "def get_quantile(data, num_buckets=50):\n",
      "    \"\"\"Computes quantiles by taking the inverse CDF\"\"\"\n",
      "    norm_dist = stats.norm()\n",
      "    normed_data = norm_dist.cdf(data)\n",
      "    hist, _ = np.histogram(normed_data, range=(0., 1.), bins=num_buckets + 1)\n",
      "    cdf = hist[::-1].cumsum()[1:]\n",
      "    pctiles = norm_dist.ppf(cdf)\n",
      "    return pctiles\n",
      "```\n",
      "The main difference here compared to my previous solution is that I'm computing the normalized version of the data beforehand rather than applying some kind of transformation afterwards. The reason behind doing things like this is because when working with continuous variables, such as text inputs, there may exist outliers that cause problems during normalization later down the line. By precomputing these transformations ahead of time, we're able\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "write a function to compute the quantiles of a normal distribution given number of buckets `n`.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "688ed75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|> You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you donâ€™t know the answer to a question, please donâ€™t share false information. <|endoftext|> <|prompter|> \n",
      "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
      " <|endoftext|> <|assistant|> Here's an example script:\n",
      "```python\n",
      "from transformers import AutoModelForCausalLM, LoraConfig, get_peft_model\n",
      "import torch\n",
      "\n",
      "\n",
      "# Load model and tokenizer\n",
      "model = AutoModelForCausalLM.from_pretrained(\"bigscience/T5-Base-v12\")\n",
      "tokenizer = model.get_input_embeddings()\n",
      "\n",
      "# Create config with desired parameters (here we use T5)\n",
      "lora_config = LoraConfig(r=32, lora_alpha=0.1, target_modules=[\"all\"]) # r stands for rank, alpha controls scaling factor\n",
      "\n",
      "# Get Peft Model by applying LoRa layer on base model\n",
      "base_model = get_peft_model(\n",
      "    model, \n",
      "    peft_type=\"ia3\", \n",
      "    adapter_name=\"my_adapter\"\n",
      ").to(\"cuda\")\n",
      "\n",
      "# Train!\n",
      "for epoch in range(num_epochs):\n",
      "    loss = train_function(...)\n",
      "    if epoch % save_checkpoint == 0:\n",
      "        checkpoint_path = f\"{save_directory}/epoch_{epoch}.ckpt\".format(...).replace(' ', '_')\n",
      "        torch.save({\n",
      "           'state_dict': base_model.state_dict(), \n",
      "           ...}, checkpoint_path)\n",
      "```\n",
      "This will create a new adapter called \"my_adapter\" inside `base_model` which contains only LoRaWAN layers. The rest of the network remains unchanged except its weights have been randomly initialized during training so far. This means it can be trained again without losing anything important. However, this approach requires more memory than just storing the full weight matrix of the original model since all the additional components needed for LoRaWAN such as matrices used for low-rank approximation etc... must also be stored alongside the main model weights. Therefore, when loading back into the same device where the initial model was loaded onto, there may be issues related to fitting those extra elements due to different shapes between them. For instance, trying to load a PyTorch LongTensor object containing integers but having shape [4] would result in error because the tensor has no dimensionality matching what is expected at some point within the data structure holding both tensors. In order to avoid these problems one needs either to change their input type to float64 before saving or convert each element separately after reading through the file contents via numpy array objects. Both approaches require further processing steps once they're read out of storage\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "Write a code snippet for using LoRA method from ðŸ¤— PEFT.\n",
    "\"\"\"\n",
    "print(get_model_pred(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b721229",
   "metadata": {},
   "source": [
    "### Test infilling and code completion ability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b0446f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>\n",
      "# Here is the correct implementation of the two sum code exercise\n",
      "# time complexity: O(N)\n",
      "# space complexity: O(N)\n",
      "def two_sum(arr, target_sum):\n",
      "<fim_suffix><fim_middle>    # initialize a dict to store pairs\n",
      "    pair = {}\n",
      "    \n",
      "    for i in range (len(arr)):\n",
      "        if arr[i] not in pair.keys():\n",
      "            pair[target-arr[i]]=arr[i]\n",
      "            \n",
      "        else:\n",
      "            return [pair[target - arr[i]], i+1]<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "# Here is the correct implementation of the two sum code exercise\n",
    "# time complexity: O(N)\n",
    "# space complexity: O(N)\n",
    "def two_sum(arr, target_sum):\n",
    "\"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix, disable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "39efbc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>\n",
      "from accelerate import Accelerator\n",
      "\n",
      "accelerator = Accelerator()\n",
      "\n",
      "model, optimizer, training_dataloader, scheduler = <fim_suffix><fim_middle>accelerator.prepare(\n",
      "    model, optimizer, train_dataloader, lr_scheduler=scheduler\n",
      ")\n",
      "\n",
      "\n",
      "def main():\n",
      "    for epoch in range(num_epochs):\n",
      "        # Train!\n",
      "        total_loss = 0\n",
      "        for step, batch in enumerate(training_dataloader):\n",
      "            outputs = model(**batch)\n",
      "            loss = outputs.loss\n",
      "            total_loss += loss.detach().float()\n",
      "\n",
      "            if (step + 1) % num_update_steps == 0:\n",
      "                optimizer.zero_grad()\n",
      "\n",
      "                with torch.no_grad():\n",
      "                    output = model(**batch).logits\n",
      "\n",
      "                labels = batch[\"labels\"]\n",
      "                logits = output[torch.arange(len(output)), labels]\n",
      "                loss = cross_entropy_loss(logits, labels) / len(labels)\n",
      "\n",
      "                accelerator.backward(loss)\n",
      "                optimizer.step()\n",
      "\n",
      "        # Print the loss at the end of each epoch\n",
      "        avg_loss = total_loss / len(train_dataset)\n",
      "        print(\"Epoch:\", epoch, \"Loss:\", round(avg_loss.item(), 4))\n",
      "\n",
      "    test_metrics = evaluate(model, eval_dataloader)\n",
      "    print(\"\\nTest metrics:\")\n",
      "    for metric, value in test_metrics.items():\n",
      "        print(f\"{metric}: {value}\")\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    main()<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model, optimizer, training_dataloader, scheduler = \"\"\"\n",
    "\n",
    "suffix = \"\"\"\"\"\"\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7fc7cee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>from peft import LoraConfig, TaskType\n",
      "\n",
      "peft_config = LoraConfig(<fim_suffix>)<fim_middle>task_type=TaskType.SEQ_2_SEQ<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "prefix = \"\"\"from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\"\"\"\n",
    "\n",
    "suffix = \")\"\n",
    "model.set_adapter(\"copilot\")\n",
    "print(get_code_completion(prefix, suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "646439a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete_adapter(\"code_buddy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3d92b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
