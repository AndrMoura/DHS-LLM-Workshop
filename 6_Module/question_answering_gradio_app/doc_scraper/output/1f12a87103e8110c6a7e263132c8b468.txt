Configuration
	
The configuration classes stores the configuration of a PeftModel, PEFT adapter models, and the configurations of PrefixTuning, PromptTuning, and PromptEncoder. They contain methods for saving and loading model configurations from the Hub, specifying the PEFT method to use, type of task to perform, and model configurations like number of layers and number of attention heads.

PeftConfigMixin
	

class peft.utils.config.PeftConfigMixin

<
source
>
(
peft_type: typing.Optional[peft.utils.config.PeftType] = None
auto_mapping: typing.Optional[dict] = None

)


Parameters 

peft_type (Union[PeftType, str]) — The type of Peft method to use.



This is the base configuration class for PEFT adapter models. It contains all the methods that are common to all
PEFT adapter models. This class inherits from PushToHubMixin which contains the methods to
push your model to the Hub. The method save_pretrained will save the configuration of your adapter model in a
directory. The method from_pretrained will load the configuration of your adapter model from a directory.

from_json_file

<
source
>
(
path_json_file
**kwargs

)


Parameters 

path_json_file (str) —
The path to the json file.



Loads a configuration file from a json file.

from_pretrained

<
source
>
(
pretrained_model_name_or_path
subfolder = None
**kwargs

)


Parameters 

pretrained_model_name_or_path (str) —
The directory or the Hub repository id where the configuration is saved.


kwargs (additional keyword arguments, optional) —
Additional keyword arguments passed along to the child class initialization.



This method loads the configuration of your adapter model from a directory.

save_pretrained

<
source
>
(
save_directory
**kwargs

)


Parameters 

save_directory (str) —
The directory where the configuration will be saved.


kwargs (additional keyword arguments, optional) —
Additional keyword arguments passed along to the push_to_hub
method.



This method saves the configuration of your adapter model in a directory.

PeftConfig
	

class peft.PeftConfig

<
source
>
(
peft_type: typing.Union[str, peft.utils.config.PeftType] = None
auto_mapping: typing.Optional[dict] = None
base_model_name_or_path: str = None
revision: str = None
task_type: typing.Union[str, peft.utils.config.TaskType] = None
inference_mode: bool = False

)


Parameters 

peft_type (Union[PeftType, str]) — The type of Peft method to use.


task_type (Union[TaskType, str]) — The type of task to perform.


inference_mode (bool, defaults to False) — Whether to use the Peft model in inference mode.



This is the base configuration class to store the configuration of a PeftModel.

PromptLearningConfig
	

class peft.PromptLearningConfig

<
source
>
(
peft_type: typing.Union[str, peft.utils.config.PeftType] = None
auto_mapping: typing.Optional[dict] = None
base_model_name_or_path: str = None
revision: str = None
task_type: typing.Union[str, peft.utils.config.TaskType] = None
inference_mode: bool = False
num_virtual_tokens: int = None
token_dim: int = None
num_transformer_submodules: typing.Optional[int] = None
num_attention_heads: typing.Optional[int] = None
num_layers: typing.Optional[int] = None

)


Parameters 

num_virtual_tokens (int) — The number of virtual tokens to use.


token_dim (int) — The hidden embedding dimension of the base transformer model.


num_transformer_submodules (int) — The number of transformer submodules in the base transformer model.


num_attention_heads (int) — The number of attention heads in the base transformer model.


num_layers (int) — The number of layers in the base transformer model.



This is the base configuration class to store the configuration of PrefixTuning, PromptEncoder, or
PromptTuning.
